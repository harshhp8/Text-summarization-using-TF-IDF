{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H:/College WOrk/papers/stop-words.txt',encoding='utf-16') as fp:\n",
    "    v = fp.read()\n",
    "\n",
    "def removestopwords(line):\n",
    "    #print(\"original line\",line)\n",
    "    querywords = line.split()\n",
    "\n",
    "    resultwords  = [word for word in querywords if word.lower() not in v]\n",
    "    result = ' '.join(resultwords)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def tokenizefortfidf(originalsen):\n",
    "    tokenized_sents = [word_tokenize(i) for i in originalsen]\n",
    "    sen_list1=[]\n",
    "    for i in tokenized_sents:\n",
    "        sen_list1=sen_list1+i\n",
    "    return sen_list1\n",
    "#print(sen_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "   \n",
    "    for word, val in docList.items():\n",
    "        idfDict[word] = math.log10(N / float(val) + 1)\n",
    "        \n",
    "    return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "def loopsecond(word1len,word1,idf,j):\n",
    "    for k in range(word1len): \n",
    "            \n",
    "            if idf.index[j] == word1[k]:\n",
    "                lst.append(idf['ratio'][j])\n",
    "               \n",
    "                break\n",
    "    \n",
    "def loopfirst(leng,word1,idf):\n",
    "    word1len = len(word1)\n",
    "    for j in range(leng):\n",
    "        #print(j)\n",
    "        loopsecond(word1len,word1,idf,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def Average(lst):\n",
    "    return mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge import rouge_n_summary_level\n",
    "from rouge import rouge_l_summary_level\n",
    "from rouge import rouge_w_sentence_level\n",
    "from rouge import rouge_w_summary_level\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "def comparerouge1(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_1 = rouge_n_summary_level(tokenize_words, tokenize_words1, 1)\n",
    "    #rouge1.append(rouge_1)\n",
    "    #print('ROUGE-1: %f' % rouge_1)\n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "    #print('ROUGE-2: %f' % rouge_2)\n",
    "    #rouge2.append(rouge_2)\n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    #print('ROUGE-L: %f' % rouge_l)\n",
    "    #rougel.append(rouge_l)\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    #avg=(rouge_1+rouge_2+rouge_l+rouge_w)/4\n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerouge2(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "   \n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "   \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougel(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    \n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougew(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "import io\n",
    "\n",
    "\n",
    "rouge1=[]\n",
    "rouge2=[]\n",
    "rougel=[]\n",
    "rougew=[]\n",
    "class ShortText:\n",
    "    def __init__(self, my_id, human_label, summary, short_text):\n",
    "        self.id = my_id         \n",
    "        self.human_label = human_label    \n",
    "        self.summary = summary \n",
    "        self.short_text = short_text\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return '%d\\t%d\\t%s\\t%s' % (self.id, self.human_label, self.summary, self.short_text)\n",
    "\n",
    "def load_file(filename,createfile):\n",
    "    tokenize_words=[]\n",
    "    originalsen=[]\n",
    "    #file1 = open(createfile, \"w\")\n",
    "    #retrieve the original text \n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    #use beautifulsoup to get tag attributes and elements\n",
    "    soup = BeautifulSoup(data)\n",
    "    docno=soup.find_all('docno')\n",
    "    \n",
    "    text=soup.find_all('text')\n",
    "    #store in a dictionary with ShortText Instances as values\n",
    "    instances = {}\n",
    "    my_id = 0\n",
    "    for n,tit in zip(docno,text):\n",
    "        #print(\"hello2\")\n",
    "        tit=tit.get_text()\n",
    "        tit=str(tit)\n",
    "        #print(\"this is text\",tit)\n",
    "        #s = re.sub('[!#?,.:\";]', '', tit)\n",
    "        for tokenize in indic_tokenize.trivial_tokenize(tit):\n",
    "            tokenize_words.append(tokenize)\n",
    "        # Convert list of strings to string\n",
    "        originalfull_str = convert_list_to_string(tokenize_words)\n",
    "        senlateron=[]\n",
    "        sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "        for t in sentences:\n",
    "            senlateron.append(t)\n",
    "        originalsen=[]\n",
    "        sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "        for t in sentences:\n",
    "            sen=removestopwords(t)\n",
    "            originalsen.append(sen)\n",
    "            #print(originalsen)\n",
    "        sen_listfortfidf=tokenizefortfidf(originalsen)\n",
    "        #print(sen_listfortfidf)\n",
    "        list3 = [''.join(c for c in s if c not in string.punctuation) for s in sen_listfortfidf]\n",
    "        list3 = [s for s in list3 if s]\n",
    "        dictionary = wordListToFreqDict(list3)\n",
    "        sorteddict = sortFreqDict(dictionary)\n",
    "        context=listToString(list3)\n",
    "        wordDictA = dict.fromkeys(list3, 0)\n",
    "        for word in list3:\n",
    "            wordDictA[word]+=1\n",
    "        df2=pd.DataFrame([wordDictA])\n",
    "        tfFirst = computeTF(wordDictA, context)\n",
    "        tf = pd.DataFrame([tfFirst])\n",
    "        idfs = computeIDF(wordDictA)\n",
    "        idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "        #idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "        #putting it in a dataframe\n",
    "        idf= pd.DataFrame([idfFirst])\n",
    "        idf = idf.transpose()\n",
    "        idf.columns=['ratio']\n",
    "        final_df = pd.DataFrame(columns=['sentence', 'score'])\n",
    "\n",
    "        for i in range(len(originalsen)):\n",
    "            originalsen[i]=str(originalsen[i])[1:-1]\n",
    "            word = listToString(originalsen[i])\n",
    "   \n",
    "            word1 = word.split()\n",
    "    \n",
    "            lenofdf=len(idf)\n",
    "            loopfirst(lenofdf,word1,idf)\n",
    "            total = sum(lst)\n",
    "            final_df = final_df.append({'sentence': originalsen[i], 'score': total},ignore_index='True')\n",
    "    \n",
    "            lst.clear()\n",
    "        final_df.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "        #final_df1=final_df.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "        #print(final_df)\n",
    "        final_df2=final_df['sentence'].head(round(50*len(final_df.index)/100))\n",
    "        #print(\"this is final_df2\",final_df2)\n",
    "        listtoretrieve=list(final_df2.index.values) \n",
    "        listtoretrieve.sort()\n",
    "        #print(listtoretrieve)\n",
    "#         #final_df.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "        #from original list retrive\n",
    "        df2 = pd.DataFrame(senlateron,columns=['sentences'])\n",
    "        new_list = df2.values.tolist()\n",
    "        res1=[]\n",
    "\n",
    "        for x in new_list:\n",
    "            res1=res1+x\n",
    "        finaldraft=\"\"\n",
    "        for i in listtoretrieve:\n",
    "            finaldraft+=res1[i]\n",
    "        finaldraft=\" \".join(finaldraft.split()) \n",
    "        rouge_1=comparerouge1(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_2=comparerouge2(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_l=comparerougel(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        rouge_w=comparerougew(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",finaldraft,filename)\n",
    "        \n",
    "        rouge1.append(rouge_1)\n",
    "        rouge2.append(rouge_2)\n",
    "        rougel.append(rouge_l)\n",
    "        rougew.append(rouge_w)\n",
    "        \n",
    "        rouge1average=Average(rouge1)\n",
    "        rouge2average=Average(rouge2)\n",
    "        rougelaverage=Average(rougel)\n",
    "        rougewaverage=Average(rougew)\n",
    "        print(\"rouge1 average\",rouge1average)\n",
    "        print(\"rouge2 average\",rouge2average)\n",
    "        print(\"rougel average\",rougelaverage)\n",
    "        print(\"rougew average\",rougewaverage)\n",
    "        #print(finaldraft)\n",
    "        with io.open(createfile, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(finaldraft)\n",
    "       \n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"H:/gujarati Text summarization dataset/economics/\"):   \n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                load_file(os.path.join(root, file),\"H:/TFIDFgujarati Text summarization dataset/economics/\"+file)\n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
